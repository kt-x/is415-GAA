---
title: "Take-Home_Ex03"
date: "9 Mar 2023"
date-modified: "`r Sys.Date()`"
number-sections: true
format: html
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
---

# Overview

The price of housing is affected by many factors such as the general economy of a country or inflation rate. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

Conventional, housing resale prices predictive models were built by using [**Ordinary Least Square (OLS)**](https://en.wikipedia.org/wiki/Ordinary_least_squares) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, **Geographical Weighted Models** were introduced for calibrating predictive model for housing resale prices.

## Data Used

| Type                      | Name                              | Format  | Source                                                                                   |
|---------------|---------------|---------------|-----------------------------|
| Aspatial                  | HDB Flat Resale Prices            | csv     | [data.gov](https://data.gov.sg/dataset/resale-flat-prices)                               |
| Geospatial                | Master Plan 2014 Subzone Boundary | shp     | [data.gov](https://data.gov.sg/dataset/master-plan-2014-subzone-boundary-web)            |
| Locational (w geo-coord)  | Eldercare services                | shp     | [data.gov](https://data.gov.sg/dataset/eldercare-services)                               |
| Locational (w geo-coord)  | Park facilities                   | geojson | [data.gov](https://data.gov.sg/dataset/park-facilities)                                  |
| Locational (w geo-coord)  | Hawker centres                    | geojson | [data.gov](https://data.gov.sg/dataset/hawker-centres)                                   |
| Locational (w geo-coord)  | Supermarkets                      | geojson | [data.gov](https://data.gov.sg/dataset/supermarkets)                                     |
| Locational (w geo-coord)  | Bus Stops                         | shp     | [Datamall.lta.gov.sg](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)  |
| Locational (w geo-coord)  | MRT                               | shp     | [Datamall.lta.gov.sg](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)  |
| Locational (w geo-coord)  | Childcare                         | csv     | [dataportal.asia](https://dataportal.asia/dataset/203030733_child-care-services)         |
| Locational (no geo-coord) | Kindergartens                     | csv     | [dataportal.asia](https://dataportal.asia/dataset/192512222_list-of-kindergartens)       |
| Locational (no geo-coord) | Primary school                    | csv     | [data.gov](https://data.gov.sg/dataset/school-directory-and-information)                 |
| Locational (no geo-coord) | CBD                               |         | Google search                                                                            |
| Locational (no geo-coord) | Shopping Malls                    | list    | [Wikipedia](https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore)           |
| Locational (no geo-coord) | Good primary school               | list    | [Local Salary Forum](https://www.salary.sg/2021/best-primary-schools-2021-by-popularity) |

## Packages

\[References taken from [Take-home Exercise 3](https://aisyahajit2018-is415.netlify.app/posts/2021-11-07-take-home-exercise-3/) by NOR AISYAH BINTE AJIT.\]

\[References taken from [Take-Home Exercise 3: Hedonic Pricing Models for Resale Prices of Public Housing in Singapore](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/) by MEGAN SIM TZE YEN.\]

-   **sf**: used for importing, managing, and processing geospatial data

-   **tidyverse**: a collection of packages for data science tasks

-   **tmap**: used for creating thematic maps, such as choropleth and bubble maps

-   **spdep**: used to create spatial weights matrix objects, global and local spatial autocorrelation statistics and related calculations (e.g. spatially lag attributes)

-   **httr**: used to make API calls, such as a GET request

-   **jsonlite**: a JSON parser that can convert from JSON to the appropraite R data types

-   **rvest: A new package that makes it easy to scrape (or harvest) data from html web pages, inspired by libraries like beautiful soup.**

    -   In this analysis, it will be used to scrape data for **Shopping malls** and **Good primary schools**

**Tidyverse packages:**

-   **readr** for importing delimited files (.csv)

-   **readxl** for importing Excel worksheets (.xlsx) - note that it has to be loaded explicitly as it is not a core tidyverse package

-   **tidyr** for manipulating and tidying data

-   **dplyr** for wrangling and transforming data

-   **ggplot2** for visualising data

**Building + visualising hedonic pricing models:**

-   **olsrr**: used for building least squares regression models

-   **coorplot** + **ggpubr**: both are used for multivariate data visualisation & analysis

-   **GWmodel**: provides a collection of localised spatial statistical methods, such as summary statistics, principal components analysis, discriminant analysis and various forms of GW regression

**Visualisations:**

-   **devtools:** used for installing any R packages which is not available in RCRAN. In this exercise, I will be installing using devtools to install the package xaringanExtra which is still under development stage.

-   **kableExtra**: an extension of kable, used for table customisation

-   **plotly**: used for creating interactive web graphics, and can be used in conjunction with ggplot2 with the `ggplotly()` function

-   **ggthemes**: an extension of ggplot2, with more advanced themes for plotting

## Import Packages

```{r}
pacman::p_load(sf, tidyverse, tmap, httr, rvest, spdep, readxl, jsonlite, olsrr, corrplot, ggpubr, GWmodel, kableExtra, plotly, ggthemes, broom, devtools, SpatialML, rsample, Metrics, ranger)
```

```{r}
#devtools::install_github("gadenbuie/xaringanExtra")
library(xaringanExtra)
```

# Aspatial Data Wrangling

\[References taken from [Take-home Exercise 3](https://aisyahajit2018-is415.netlify.app/posts/2021-11-07-take-home-exercise-3/) by NOR AISYAH BINTE AJIT.\]

```{r}
resale <- read_csv("data/aspatial/resale-flat-prices.csv")
glimpse(resale)
```

## Filter the data

The study should focus on either three-room, four-room or five-room flat and transaction period should be from 1st January 2021 to 31st December 2022. The test data should be January and February 2023 resale prices.

For this project, I will be going with 4 room flats.

::: panel-tabset
#### Code

```{r}
rs_subset <-  filter(resale,flat_type == "4 ROOM") %>%
                    filter(month >= "2021-01" & month <= "2023-02")
```

#### Glimpse

```{r}
glimpse(rs_subset)
```

#### Unique month

```{r}
unique(rs_subset$month)
```

#### Unique flat_type

```{r}
unique(rs_subset$flat_type)
```
:::

## Transform resale data

After checking the correctly filtered out data, next is transforming the data,

### New columns

::: panel-tabset
#### Code

```{r}
rs_transform <- rs_subset %>%
  mutate(rs_subset, address = paste(block,street_name)) %>%
  mutate(rs_subset, remaining_lease_yr = as.integer(str_sub(remaining_lease, 0, 2))) %>%
  mutate(rs_subset, remaining_lease_mth = as.integer(str_sub(remaining_lease, 9, 11)))
```

#### Head

```{r}
head(rs_transform)
```
:::

### Add up the remaining lease in months

-   Replace NA values in remaining_lease_mth with the value 0 using is.na() function
-   Multiply remaining_lease_yr by 12 to convert it to months unit
-   Create remaining_lease_mths column using mutate function of dplyr package which contains the summation of the remaining_lease_yr and remaining_lease_mths using rowSums() function of base R package
-   Select required columns for analysis

::: panel-tabset
#### Code

```{r}
rs_transform$remaining_lease_mth[is.na(rs_transform$remaining_lease_mth)] <- 0
rs_transform$remaining_lease_yr <- rs_transform$remaining_lease_yr * 12
rs_transform <- rs_transform %>% 
  mutate(rs_transform, remaining_lease_mths = rowSums(rs_transform[, c("remaining_lease_yr", "remaining_lease_mth")])) %>%
  select(month, town, address, block, street_name, flat_type, storey_range, floor_area_sqm, flat_model, 
         lease_commence_date, remaining_lease_mths, resale_price)
```

#### Head

```{r}
head(rs_transform)
```
:::

## Retrieve Postal Codes and Coordinates of Addresses

This section aims to get the postal codes and coordinates which is needed for the locational factors without geographical coordinates.

### Create a list storing unique addresses

-   Use unique() function to extract the unique addresses then use sort() function to sort the unique vector.

```{r}
#|eval: false
add_list <- sort(unique(rs_transform$address))
```

## Create function to retrieve coordinates from OneMap.Sg API

```{r}
#| eval: false
get_coords <- function(add_list){
  
  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (i in add_list){
    #print(i)

    r <- GET('https://developers.onemap.sg/commonapi/search?',
           query=list(searchVal=i,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
    }
    
    # If multiple results, drop NIL and append top 1
    else if (found > 1){
      # Remove those with NIL as postal
      res_sub <- res[res$POSTAL != "NIL", ]
      
      # Set as NA first if no Postal
      if (nrow(res_sub) == 0) {
          new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
      }
      
      else{
        top1 <- head(res_sub, n = 1)
        postal <- top1$POSTAL 
        lat <- top1$LATITUDE
        lng <- top1$LONGITUDE
        new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
      }
    }

    else {
      new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  return(postal_coords)
}
```

### Call get_coords function to retrieve resale coordinates

```{r}
#| eval: false
coords <- get_coords(add_list)
```

### Inspect results

```{r}
#| eval: false
coords[(is.na(coords$postal) | is.na(coords$latitude) | is.na(coords$longitude) | coords$postal=="NIL"), ]
```

It seems like there is 1 address that does not contain geographic coordinates. After further investigation, I will be input in

```{r}
#| eval: false
coords$postal[coords$postal=="NIL"] <- "680215"
```

```{r}
#| eval: false
coords[(is.na(coords$postal) | is.na(coords$latitude) | is.na(coords$longitude) | coords$postal=="NIL"), ]
```

great, no more NIL postal codes.

### Combine resale and coordinates data

Now, we need to combine the data

::: panel-tabset
#### Code

```{r}
#| eval: false
rs_coords <- left_join(rs_transform, coords, by = c('address' = 'address'))
```

#### Head

```{r}
#| eval: false
head(rs_coords)
```
:::

### Handle invalid addresses

By replacing sub string in invalid addresses in the address and extract rows with addresses containing SAINT GEORGE'S

::: panel-tabset
#### Code

```{r}
#| eval: false
rs_coords$address <- sub("ST. GEORGE'S", "SAINT GEORGE'S", rs_coords$address)
rs_invalid <- rs_coords[grepl("SAINT GEORGE'S", rs_coords$address), ]
```

#### Glimpse

```{r}
#| eval: false
glimpse(rs_invalid)
```
:::

There are 32 rows that contains SAINT GEORGE'S as street name but has a substring replaced in the address.

#### Create unique list of addresses again

```{r}
#| eval: false
add_list <- sort(unique(rs_invalid$address))
```

#### Call get_coords to retrieve resale coordinates again

```{r}
#| eval: false
rs_invalid_coords <- get_coords(add_list)
```

#### Inspect results again

```{r}
#| eval: false
rs_invalid_coords[(is.na(rs_invalid_coords$postal) | is.na(rs_invalid_coords$latitude) | is.na(rs_invalid_coords$longitude)), ]
```

So the results shows no invalid coordinates now.

#### Combine rs_invalid_coords with rs_coords data

::: panel-tabset
##### Code

```{r}
#| eval: false
rs_coords_final <- rs_coords %>%
  left_join(rs_invalid_coords, by = c("address")) %>%
  mutate(latitude = ifelse(is.na(postal.x), postal.y, postal.x)) %>%
  mutate(latitude = ifelse(is.na(latitude.x), latitude.y, latitude.x)) %>%
  mutate(longitude = ifelse(is.na(longitude.x), longitude.y, longitude.x)) %>%
  select(-c(postal.x, latitude.x, longitude.x, postal.y, latitude.y, longitude.y))
```

##### Head

```{r}
#| eval: false
head(rs_coords_final)
```
:::

## Write file to rds

```{r}
#| eval: false
rs_coords_rds <- write_rds(rs_coords_final, "data/aspatial/rds/rs_coords.rds")
```

## Read rs_coords RDS file

::: panel-tabset
#### Code

```{r}
rs_coords <- read_rds("data/aspatial/rds/rs_coords.rds")
```

#### Glimpse

```{r}
glimpse(rs_coords)
```
:::

### Assign and Transform CRS and Check

::: panel-tabset
#### Code

```{r}
rs_coords_sf <- st_as_sf(rs_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

#### st_crs

```{r}
st_crs(rs_coords_sf)
```
:::

### Check for invalid geometries

```{r}
length(which(st_is_valid(rs_coords_sf) == FALSE))
```

No invalid geometries

### Plot hdb resale points

```{r}
tmap_mode("view")
tm_shape(rs_coords_sf)+
  tm_dots(col="blue", size = 0.02)
  tm_view(set.zoom.limits = c(6,8))
tmap_mode("plot")
```

# Locational with geographical coordinates

## Read and check CRS of Locational factors

::: panel-tabset
#### elder_sf

```{r}
elder_sf <- st_read(dsn = "data/geospatial/eldercare-services", layer = "ELDERCARE")
```

##### st_crs

```{r}
st_crs(elder_sf)
```

#### parks_sf

```{r}
parks_sf <- st_read("data/geospatial/park-facilities/park-facilities-geojson.geojson") 
```

##### st_crs

```{r}
st_crs(parks_sf)
```

#### hawker_sf

```{r}
hawker_sf <- st_read("data/geospatial/hawker-centres/hawker-centres-geojson.geojson") 
```

##### st_crs

```{r}
st_crs(hawker_sf)
```

#### supermkt_sf

```{r}
supermkt_sf <- st_read("data/geospatial/supermarkets/supermarkets-geojson.geojson")
```

##### st_crs

```{r}
st_crs(supermkt_sf)
```

#### childcare_sf

```{r}
childcare_sf <- st_read("data/geospatial/childcare/childcare.geojson")
```

##### st_crs

```{r}
st_crs(childcare_sf)
```

#### kind_sf

```{r}
kind_sf <- st_read("data/geospatial/kindergartens/preschools-location.geojson")
```

##### st_crs

```{r}
st_crs(kind_sf)
```

#### mrt_sf

```{r}
mrtlrt_sf <- st_read(dsn = "data/geospatial/TrainStation", layer="Train_Station_Exit_Layer")
```

##### st_crs

```{r}
st_crs(mrtlrt_sf)
```

#### bus_sf

```{r}
bus_sf <- st_read(dsn = "data/geospatial/BusStopLocation", layer="BusStop")
```

##### st_crs

```{r}
st_crs(bus_sf)
```
:::

### Assign EPSG code to sf dataframes and check again

::: panel-tabset
#### Code

```{r}
elder_sf <- st_set_crs(elder_sf, 3414)
mrtlrt_sf <- st_set_crs(mrtlrt_sf, 3414)
bus_sf <- st_set_crs(bus_sf, 3414)

hawker_sf <- hawker_sf %>%
  st_transform(crs = 3414)
parks_sf <- parks_sf %>%
  st_transform(crs = 3414)
supermkt_sf <- supermkt_sf %>%
  st_transform(crs = 3414)
childcare_sf <- childcare_sf %>%
  st_transform(crs = 3414)
kind_sf <- kind_sf %>%
  st_transform(crs = 3414)
```

#### st_crs

```{r}
st_crs(elder_sf)
st_crs(mrtlrt_sf)
st_crs(bus_sf)
st_crs(hawker_sf)
st_crs(parks_sf)
st_crs(supermkt_sf)
st_crs(childcare_sf)
st_crs(kind_sf)
```
:::

## Check for invalid geometries

```{r}
length(which(st_is_valid(elder_sf) == FALSE))
length(which(st_is_valid(mrtlrt_sf) == FALSE))
length(which(st_is_valid(hawker_sf) == FALSE))
length(which(st_is_valid(parks_sf) == FALSE))
length(which(st_is_valid(supermkt_sf) == FALSE))
length(which(st_is_valid(childcare_sf) == FALSE))
length(which(st_is_valid(kind_sf) == FALSE))
length(which(st_is_valid(bus_sf) == FALSE))
```

NO invalid geometries.

## Calculate Proximity

```{r}
#| eval: false
get_prox <- function(origin_df, dest_df, col_name){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)           
  
  # find the nearest location_factor and create new data frame
  near <- origin_df %>% 
    mutate(PROX = apply(dist_matrix, 1, function(x) min(x)) / 1000) 
  
  # rename column name according to input parameter
  names(near)[names(near) == 'PROX'] <- col_name

  # Return df
  return(near)
}
```

#### CALL GET_PROX FUNCTION

```{r}
#| eval: false
rs_coords_sf <- get_prox(rs_coords_sf, elder_sf, "PROX_ELDERLYCARE")
rs_coords_sf <- get_prox(rs_coords_sf, mrtlrt_sf, "PROX_MRT")
rs_coords_sf <- get_prox(rs_coords_sf, hawker_sf, "PROX_HAWKER")
rs_coords_sf <- get_prox(rs_coords_sf, parks_sf, "PROX_PARK")
rs_coords_sf <- get_prox(rs_coords_sf, supermkt_sf, "PROX_SUPERMARKET")
```

## Create get_within function to calculate no. of factors within dist

```{r}
#| eval: false
get_within <- function(origin_df, dest_df, threshold_dist, col_name){

  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)

  # count the number of location_factors within threshold_dist and create new data frame
  wdist <- origin_df %>%
    mutate(WITHIN_DT = apply(dist_matrix, 1, function(x) sum(x <= threshold_dist)))

  # rename column name according to input parameter
  names(wdist)[names(wdist) == 'WITHIN_DT'] <- col_name

  # Return df
  return(wdist)
}
```

#### CALL GET_WITHIN FUNCTION

The threshold will be set to 350m for locational factors such as, Kindergartens, Childcare centres and Bus stops.

##### kindergarten

::: panel-tabset
###### Code

```{r}
#| eval: false
rs_coords_sf <- get_within(rs_coords_sf, kind_sf, 350, "WITHIN_350M_KINDERGARTEN")
```

###### Head

```{r}
#| eval: false
head(rs_coords_sf)
```
:::

##### Childcare centres

::: panel-tabset
###### Code

```{r}
#| eval: false
rs_coords_sf <- get_within(rs_coords_sf, childcare_sf, 350, "WITHIN_350M_CHILDCARE")
```

###### Head

```{r}
#| eval: false
head(rs_coords_sf)
```
:::

##### Bus stops

::: panel-tabset
###### Code

```{r}
#| eval: false
rs_coords_sf <- get_within(rs_coords_sf, bus_sf, 350, "WITHIN_350M_BUS")
```

###### Head

```{r}
#| eval: false
head(rs_coords_sf)
```
:::

# Locational factors without geographic coordinates

## CBD

according to google, the latitude and longitude of Downtown Core (aka CBD), are 1.287953 and 103.851784 respectively.

#### STORE CBD COORDINATES IN DATAFRAME

```{r}
#| eval: false
name <- c('CBD Area')
latitude= c(1.287953)
longitude= c(103.851784)
cbd_coords <- data.frame(name, latitude, longitude)
```

#### ASSIGN AND TRANSFORM CRS

To convert the data frame into sf object then, transform the coordinates of the sf object

::: panel-tabset
##### code

```{r}
#| eval: false
cbd_coords_sf <- st_as_sf(cbd_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) |>
  st_transform(crs = 3414)
```

##### st_crs

```{r}
#| eval: false
st_crs(cbd_coords_sf)
```
:::

The coordinates for CBD area is in EPSG 3414 (SVY21) format is c(30055.05, 30040.83).

#### CALL GET_PROX FUNCTION

Get the proximity of HDB and CBD area.

```{r}
#| eval: false
rs_coords_sf <- get_prox(rs_coords_sf, cbd_coords_sf, "PROX_CBD") 
```

## Shopping mall

In a nutshell, the following code chunk will perform 3 steps: - Read the Wikipedia html page containing the Shopping Malls in Singapore - Read the text portion (html_text()) of the Unordered - - List element selected by html_nodes() - Append it to the empty mall_list created \#### EXTRACT SHOPPING MALLS FROM WIKIPEDIA

::: panel-tabset
##### Code

```{r}
#| eval: false
url <- "https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore"
malls_list <- list()

for (i in 2:7){
  malls <- read_html(url) %>%
    html_nodes(xpath = paste('//*[@id="mw-content-text"]/div[1]/div[',as.character(i),']/ul/li',sep="") ) %>%
    html_text()
  malls_list <- append(malls_list, malls)
}
```

##### st_crs

```{r}
#| eval: false
st_crs(malls_list)
```
:::

#### CALL GET_COORDS FUNCTION

To retrieve coordinates of Shopping Malls and rename the address to mall_name for easier reference

```{r}
#| eval: false
malls_list_coords <- get_coords(malls_list) %>% 
  rename("mall_name" = "address")
```

#### REMOVE INVALID SHOPPING MALL NAME

```{r}
#| eval: false
malls_list_coords <- subset(malls_list_coords, mall_name!= "Yew Tee Shopping Centre")
```

#### CORRECT INVALID MALL NAMES THAT CAN BE FOUND

```{r}
#| eval: false
invalid_malls<- subset(malls_list_coords, is.na(malls_list_coords$postal))
invalid_malls_list <- unique(invalid_malls$mall_name)
corrected_malls <- c("Clarke Quay", "City Gate", "Raffles Holland V", "Knightsbridge", "Mustafa Centre", "GR.ID", "Shaw House",
                     "The Poiz Centre", "Velocity @ Novena Square", "Singapore Post Centre", "PLQ Mall", "KINEX", "The Grandstand")

for (i in 1:length(invalid_malls_list)) {
  malls_list_coords <- malls_list_coords %>% 
    mutate(mall_name = ifelse(as.character(mall_name) == invalid_malls_list[i], corrected_malls[i], as.character(mall_name)))
}
```

#### CREATE A LIST STORING UNIQUE MALL NAMES

```{r}
#| eval: false
malls_list <- sort(unique(malls_list_coords$mall_name))
```

#### CALL GET_COORDS TO RETRIEVE COORDINATES OF SHOPPING MALLS AGAIN

```{r}
#| eval: false
malls_coords <- get_coords(malls_list)
```

#### INSPECT RESULTS

```{r}
#| eval: false
malls_coords[(is.na(malls_coords$postal) | is.na(malls_coords$latitude) | is.na(malls_coords$longitude)), ]
```

#### CONVERT DATA FRAME INTO SF OBJECT, ASSIGN AND TRANSFORM CRS

```{r}
#| eval: false
malls_sf <- st_as_sf(malls_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

#### CALL GET_PROX FUNCTION

```{r}
#| eval: false
rs_coords_sf <- get_prox(rs_coords_sf, malls_sf, "PROX_MALL") 
```

## Primary Schools

### READ IN CSV FILE

::: panel-tabset
#### code

```{r}
pri_sch <- read_csv("data/geospatial/prisch/general-information-of-schools.csv")
```

#### glimpse

```{r}
glimpse(pri_sch)
```
:::

### EXTRACT PRIMARY SCHOOLS AND REQUIRED COLUMNS ONLY

::: panel-tabset
#### code

```{r}
#| eval: false
pri_sch <- pri_sch %>%
  filter(mainlevel_code == "PRIMARY") %>%
  select(school_name, address, postal_code, mainlevel_code)
```

#### glimpse

```{r}
#| eval: false
glimpse(pri_sch)
```
:::

#### CREATE LIST STORING UNIQUE POSTAL CODES OF PRIMARY SCHOOLS

```{r}
#| eval: false
prisch_list <- sort(unique(pri_sch$postal_code))
```

#### CALL GET_COORDS FUNCTION TO RETRIEVE COORDINATES OF PRIMARY SCHOOLS

```{r}
#| eval: false
prisch_coords <- get_coords(prisch_list)
```

#### INSPECT RESULTS

```{r}
#| eval: false
prisch_coords[(is.na(prisch_coords$postal) | is.na(prisch_coords$latitude) | is.na(prisch_coords$longitude)), ]
```

No NA values.

#### COMBINE COORDINATES WITH PRIMARY SCHOOL NAMES

To verify whether we have extracted it correctly.

::: panel-tabset
##### code

```{r}
#| eval: false
prisch_coords = prisch_coords[c("postal","latitude", "longitude")]
pri_sch <- left_join(pri_sch, prisch_coords, by = c('postal_code' = 'postal'))
```

##### head

```{r}
#| eval: false
head(pri_sch)
```
:::

#### CONVERT PRI_SCH DATA FRAME INTO SF OBJECT, ASSIGN AND TRANSFORM CRS

::: panel-tabset
##### code

```{r}
#| eval: false
prisch_sf <- st_as_sf(pri_sch,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

##### st_crs

```{r}
#| eval: false
st_crs(prisch_sf)
```
:::

#### CALL GET_WITHIN FUNCTION

::: panel-tabset
##### code

```{r}
#| eval: false
rs_coords_sf <- get_within(rs_coords_sf, prisch_sf, 1000, "WITHIN_1KM_PRISCH")
```

##### head

```{r}
#| eval: false
head(rs_coords_sf)
```
:::

## Good primary school

### EXTRACT RANKING LIST OF PRIMARY SCHOOLS

::: panel-tabset
#### code

```{r}
#| eval: false
url <- "https://www.salary.sg/2021/best-primary-schools-2021-by-popularity/"

good_pri <- data.frame()

schools <- read_html(url) %>%
  html_nodes(xpath = paste('//*[@id="post-3068"]/div[3]/div/div/ol/li') ) %>%
  html_text() 

for (i in (schools)){
  sch_name <- toupper(gsub(" – .*","",i))
  sch_name <- gsub("\\(PRIMARY SECTION)","",sch_name)
  sch_name <- trimws(sch_name)
  new_row <- data.frame(pri_sch_name=sch_name)
  # Add the row
  good_pri <- rbind(good_pri, new_row)
}

top_good_pri <- head(good_pri, 10)
```

#### head

```{r}
#| eval: false
head(top_good_pri)
```
:::

#### CHECK FOR GOOD PRIMARY SCHOOLS IN PRIMARY SCHOOL DF

To check whether the names of the good primary schools is similar to the names in primary school dataframe

```{r}
#| eval: false
top_good_pri$pri_sch_name[!top_good_pri$pri_sch_name %in% prisch_sf$school_name]
```

#### CREATE A LIST STORING UNIQUE GOOD PRIMARY SCHOOL NAMES

```{r}
#| eval: false
good_pri_list <- unique(top_good_pri$pri_sch_name)
```

#### CALL GET_COORDS FUNCTION TO RETRIEVE COORDINATES OF GOOD PRIMARY SCHOOLS

```{r}
#| eval: false
goodprisch_coords <- get_coords(good_pri_list)
```

#### INSPECT RESULTS

```{r}
#| eval: false
goodprisch_coords[(is.na(goodprisch_coords$postal) | is.na(goodprisch_coords$latitude) | is.na(goodprisch_coords$longitude)), ]
```

#### REPLACE INVALID GOOD PRIMARY SCHOOL NAMES

```{r}
#| eval: false
top_good_pri$pri_sch_name[top_good_pri$pri_sch_name == "CHIJ ST. NICHOLAS GIRLS’ SCHOOL"] <- "CHIJ SAINT NICHOLAS GIRLS' SCHOOL"
top_good_pri$pri_sch_name[top_good_pri$pri_sch_name == "ST. HILDA’S PRIMARY SCHOOL"] <- "SAINT HILDA'S PRIMARY SCHOOL"
```

#### CREATE A LIST STORING UNIQUE GOOD PRIMARY SCHOOL NAMES AGAIN

```{r}
#| eval: false
good_pri_list <- unique(top_good_pri$pri_sch_name)
```

#### CALL GET_COORDS FUNCTION TO RETRIEVE COORDINATES OF GOOD PRIMARY SCHOOLS AGAIN

```{r}
#| eval: false
goodprisch_coords <- get_coords(good_pri_list)
```

#### INSPECT RESULTS AGAIN

```{r}
#| eval: false
goodprisch_coords[(is.na(goodprisch_coords$postal) | is.na(goodprisch_coords$latitude) | is.na(goodprisch_coords$longitude)), ]
```

#### CONVERT DATA FRAME INTO SF OBJECTS, ASSIGN AND TRANSFORM CRS

::: panel-tabset
##### code

```{r}
#| eval: false
goodpri_sf <- st_as_sf(goodprisch_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

##### st_crs

```{r}
#| eval: false
st_crs(goodpri_sf)
```
:::

#### CALL GET_PROX FUNCTION

```{r}
#| eval: false
rs_coords_sf <- get_prox(rs_coords_sf, goodpri_sf, "PROX_GOOD_PRISCH")
```

## Write to RDS file

To prevent running of the codes

```{r}
#| eval: false
rs_factors_rds <- write_rds(rs_coords_sf, "data/aspatial/rds/rs_factors.rds")
```

# Import Data for Analysis

## Geospatial data

### MP2019

::: panel-tabset
#### Code

```{r}
mpsz_sf <- st_read(dsn = "data/geospatial/mpsz", layer="MPSZ-2019")
```

#### st_crs()

```{r}
st_crs(mpsz_sf)
```
:::

### Transform CRS

::: panel-tabset
#### Code

```{r}
mpsz_sf <- st_transform(mpsz_sf, 3414)
```

#### st_crs()

```{r}
st_crs(mpsz_sf)
```
:::

### Remove invalid geometries (if any)

#### CHECK FOR INVALID GEOMETRIES

```{r}
length(which(st_is_valid(mpsz_sf) == FALSE))
```

#### HANDLE INVALID GEOMETRIES AND CHECK

```{r}
mpsz_sf <- st_make_valid(mpsz_sf)
length(which(st_is_valid(mpsz_sf) == FALSE))
```

### Reveal the extent of mpsz_sf

```{r}
st_bbox(mpsz_sf)
```

# Data preparation - Resale with locational factors

## Read RDS file

::: panel-tabset
#### Code

```{r}
rs_sf <- read_rds("data/aspatial/rds/rs_factors.rds")
```

#### Glimpse

```{r}
glimpse(rs_sf)
```
:::

### Extract unique storey_range and sort

```{r}
storeys <- sort(unique(rs_sf$storey_range))
```

### Create dataframe storey_range_order to store order of storey_range

::: panel-tabset
#### Code

```{r}
storey_order <- 1:length(storeys)
storey_range_order <- data.frame(storeys, storey_order)
```

#### Head

```{r}
head(storey_range_order)
```
:::

### Combine storey_order with resale dataframe

::: panel-tabset
#### Code

```{r}
rs_sf <- left_join(rs_sf, storey_range_order, by= c("storey_range" = "storeys"))
```

#### Glimpse

```{r}
glimpse(rs_sf)
```
:::

### Select required columns for analysis

::: panel-tabset
#### Code

```{r}
rs_req <- rs_sf %>%
  select(month, resale_price, floor_area_sqm, storey_order, remaining_lease_mths,
         PROX_CBD, PROX_ELDERLYCARE, PROX_HAWKER, PROX_MRT, PROX_PARK, PROX_GOOD_PRISCH, PROX_MALL,
         PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, WITHIN_350M_BUS, WITHIN_1KM_PRISCH)
```

#### Glimpse

```{r}
glimpse(rs_req)
```
:::

### View summary

```{r}
summary(rs_req)
```

## statistical graphics

### Plot Histogram of resale_price

```{r}
ggplot(data=rs_req, aes(x=`resale_price`)) +
  geom_histogram(bins=20, color="black", fill="light coral")
```

### Normalise using Log Transformation

```{r}
rs_req <- rs_req %>%
  mutate(`LOG_SELLING_PRICE` = log(resale_price))
```

### Plot Histogram of LOG_RESALE_PRICE

```{r}
ggplot(data=rs_req, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light green")
```

# Exploratory Data Analysis (EDA)

## Multiple Histogram Plots distribution of variables

### Stuctural Factors

#### EXTRACT COLUMN NAMES TO PLOT

```{r}
s_factor <- c("floor_area_sqm", "storey_order", "remaining_lease_mths")
```

#### CREATE A LIST TO STORE HISTOGRAMS OF STUCTURAL FACTORS

```{r}
s_factor_hist_list <- vector(mode = "list", length = length(s_factor))
for (i in 1:length(s_factor)) {
  hist_plot <- ggplot(rs_req, aes_string(x = s_factor[[i]])) +
    geom_histogram(color="firebrick", fill = "light coral") +
    labs(title = s_factor[[i]]) +
    theme(plot.title = element_text(size = 10),
          axis.title = element_blank())
  
  s_factor_hist_list[[i]] <- hist_plot
}
```

#### PLOT HISTOGRAMS TO EXAMINE DISTRIBUTION OF STUCTURAL FACTORS

```{r}
ggarrange(plotlist = s_factor_hist_list,
          ncol = 2,
          nrow = 2)
```

### Locational Factors

#### EXTRACT COLUMN NAMES TO PLOT

```{r}
l_factor <- c("PROX_CBD", "PROX_ELDERLYCARE", "PROX_HAWKER", "PROX_MRT", "PROX_PARK", "PROX_GOOD_PRISCH", "PROX_MALL",
              "PROX_SUPERMARKET", "WITHIN_350M_KINDERGARTEN", "WITHIN_350M_CHILDCARE", "WITHIN_350M_BUS", "WITHIN_1KM_PRISCH")
```

#### CREATE A LIST TO STORE HISTOGRAMS OF LOCATIONAL FACTORS

```{r}
l_factor_hist_list <- vector(mode = "list", length = length(l_factor))
for (i in 1:length(l_factor)) {
  hist_plot <- ggplot(rs_req, aes_string(x = l_factor[[i]])) +
    geom_histogram(color="midnight blue", fill = "light sky blue") +
    labs(title = l_factor[[i]]) +
    theme(plot.title = element_text(size = 10),
          axis.title = element_blank())
  
  l_factor_hist_list[[i]] <- hist_plot
}
```

#### PLOT HISTOGRAMS TO EXAMINE DISTRIBUTION OF LOCATIONAL FACTORS

```{r}
ggarrange(plotlist = l_factor_hist_list,
          ncol = 4,
          nrow = 4)
```

## Statistical Point Map

```{r}
tmap_mode("view")
tm_shape(rs_sf) +  
  tm_dots(col = "resale_price",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14)) +
tm_basemap("OpenStreetMap")

```

```{r}
tmap_mode("plot")
```

# Train & Test Data

### Split train and test data sets

-   Train data - Jan 2021 to Dec 2022
-   Test data - Jan 2023 to feb 2023

```{r}
#| eval: false
train_data <- rs_req |>
  filter(rs_req$month >= '2021-01' & rs_req$month <= '2022-12')

test_data <- rs_req |>
  filter(rs_req$month >= '2023-01' & rs_req$month <= '2023-02')
```

### write train and test to rds files

```{r}
#| eval: false
write_rds(train_data, "data/aspatial/rds/train_data.rds")
write_rds(test_data, "data/aspatial/rds/test_data.rds")
```

```{r}
#| eval: false
rs_req_nogeo <- st_set_geometry(rs_req, NULL) 
```

```{r}
#| eval: false
glimpse(rs_req_nogeo)
```

## Computing Correlation Matrix

```{r}
rs_req_nogeo <- rs_req %>%
  st_drop_geometry()
corrplot::corrplot(cor(rs_req_nogeo[, 2:17]), 
                   diag = FALSE, 
                   order = "AOE",
                   tl.pos = "td", 
                   tl.cex = 0.8, 
                   method = "number", 
                   type = "upper")

```

## Retriving the Stored Data

```{r}
train_data <- read_rds("data/aspatial/rds/train_data.rds")
test_data <- read_rds("data/aspatial/rds/test_data.rds")
```

## Building a non-spatial multiple linear regression

```{r}
#| eval: false
price_mlr <- lm(resale_price ~ floor_area_sqm +
                  storey_order + remaining_lease_mths +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                data=train_data)
summary(price_mlr)
```

## write price_mlr to rds file

```{r}
#| eval: false
write_rds(price_mlr, "data/aspatial/rds/price_mlr.rds" ) 
```

# gwr predictive method

In this section, you will learn how to calibrate a model to predict HDB resale price by using geographically weighted regression method of GWmodel package.

## Converting the sf data.frame to SpatialPointDataFrame

```{r}
#| eval: false
train_data_sp <- as_Spatial(train_data)
train_data_sp
```

## Computing adaptive bandwidth

Next, bw.gwr() of GWmodel package will be used to determine the optimal bandwidth to be used.

-   To determine adaptive bandwidth and CV method is used to determine the optimal bandwidth.

```{r}
#| eval: false
bw_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +
                  storey_order + remaining_lease_mths +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                  data=train_data_sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)
```

![](data/bw_adaptive.JPG)

## write bw_adpative to rds file

```{r}
#| eval: false
write_rds(bw_adaptive, "data/aspatial/rds/bw_adaptive.rds")
```

```{r}
bw_adaptive <- read_rds("data/aspatial/rds/bw_adaptive.rds")
```

## Calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel

```{r}
#| eval: false
gwr_adaptive <- gwr.basic(formula = resale_price ~
                            floor_area_sqm + storey_order +
                            remaining_lease_mths + PROX_CBD + 
                            PROX_ELDERLYCARE + PROX_HAWKER +
                            PROX_MRT + PROX_PARK + PROX_MALL + 
                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                            WITHIN_1KM_PRISCH,
                          data=train_data_sp,
                          bw=bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE,
                          longlat = FALSE)
```

![](data/gwr_adaptive_part1.JPG)

![](data/gwr_adaptive_part2.JPG)

![](data/gwr_adaptive_part3.JPG){width="344"}

![](data/gwr_adaptive_part4.JPG){width="467"}

![](data/gwr_adaptive_part5.JPG)

![](data/gwr_adaptive_part6.JPG){width="340"}

![](data/gwr_adaptive_part7.JPG)

Overall...

### save to rds

```{r}
#| eval: false
write_rds(gwr_adaptive, "data/aspatial/rds/gwr_adaptive.rds")
```

## Output

```{r}
#| eval: false
gwr_adaptive <- read_rds("data/aspatial/rds/gwr_adaptive.rds")
gwr_adaptive
```

# Preparing coordinates data

## Extracting coordinates data

extract the x,y coordinates of the full, training and test data sets.

```{r}
coords_train <- st_coordinates(train_data)
coords_test <- st_coordinates(test_data)
```

### write all the output into rds for future used

```{r}

coords_train <- write_rds(coords_train, "data/aspatial/rds/coords_train.rds" )
coords_test <- write_rds(coords_test, "data/aspatial/rds/coords_test.rds" )
```

## Droping geometry field

```{r}

train_data <- train_data %>% 
  st_drop_geometry()
```

# Calibrating Random Forest Model

In this section, we will be calibrating a model to predict HDB resale price by using random forest function of ranger package.

```{r}

set.seed(1234)
rf <- ranger(resale_price ~ floor_area_sqm + storey_order + 
               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + 
               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + 
               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + 
               WITHIN_1KM_PRISCH,
             data=train_data)
```

```{r}

print(rf)
```

## Calibrating Geographical Random Forest Model

In this section, we will be calibrating a model to predict HDB resale price by using grf() of SpatialML package.

### Calibrating using training data

Calibrate a geographic ranform forest model by using grf() of SpatialML package.

```{r}
#| eval: false
set.seed(1234)
gwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,
                     dframe=train_data, 
                     bw=55,
                     kernel="adaptive",
                     coords=coords_train)
```

## write to rds file

```{r}
#| eval: false
write_rds(gwRF_adaptive, "data/model/gwRF_adaptive.rds")
```

## retrieve for future use

```{r}
#| eval: false
gwRF_adaptive <- read_rds("data/model/gwRF_adaptive.rds")
```

## Predicting by using test data

#### prepare data

```{r}
#| eval: false
test_data <- cbind(test_data, coords_test) %>%
  st_drop_geometry()
```

Next, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.

```{r}
#| eval: false
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

### write to rds file

```{r}
#| eval: false
GRF_pred <- write_rds(gwRF_pred, "data/model/GRF_pred.rds")
```

## Converting the predicting output into a data frame

The output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.

```{r}
#| eval: false
GRF_pred <- read_rds("data/model/GRF_pred.rds")
GRF_pred_df <- as.data.frame(GRF_pred)
```

cbind() is used to append the predicted values onto test_data

```{r}
#| eval: false
test_data_p <- cbind(test_data, GRF_pred_df)
```

```{r}
#| eval: false
write_rds(test_data_p, "data/model/test_data_p.rds")
```

# Calculating Root Mean Square Error

The root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.

```{r}
#| eval: false
rmse(test_data_p$resale_price, 
     test_data_p$GRF_pred)
```

## Visualising the predicted values

A better predictive model should have the scatter point close to the diagonal line. The scatter plot can be also used to detect if any outliers in the model.

```{r}
#| eval: false
ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = resale_price)) +
  geom_point()
```

# OLS

```{r}
#| eval: false
rs_mlr1 <- lm(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL  + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, data=rs_req)


ols_regress(rs_mlr1)
```

# Hedonic Pricing Modelling in R

In this section, we will be performing regression analysis to see the relationship between a dependent variable (y) and one or more independent variables (x).

-   y variable - HDB resale price
-   x variables - the structural and locational factors

The objective is to find out the amount change in price given a one unit change in the factors.

We will also be performing both Simple and Multiple Linear Regression (MLR) Analysis to compare both results.

## Simple Linear Regression (SLR) Model

### Combine structural and locational factors list

```{r}
#| eval: false
factors <- c(s_factor, l_factor)
factors
```

### Build Simple Linear Regression model

::: panel-tabset
#### Code

```{r}
#| eval: false
intercept_df <- data.frame()
rsq_fstat_df <- data.frame()

for (i in factors){
  rs_slr <- lm(as.formula(paste("resale_price", "~", i)), data = rs_req)
  intercept <- tidy(summary(rs_slr))
  intercept$var_name <- i
  rsq_fstat <- glance(rs_slr)[1:5]
  rsq_fstat$var_name <- i
  # Append
  intercept_df <- bind_rows(intercept_df, intercept)
  rsq_fstat_df <- bind_rows(rsq_fstat_df, rsq_fstat)
}
```

#### Intercept

```{r}
#| eval: false
intercept_df
```

#### r-sq

```{r}
#| eval: false
rsq_fstat_df
```
:::

From the result,...

### Visualise best fit curve

```{r}
#| eval: false
scatterplot_list <- vector(mode = "list", length = length(factors))

for (i in factors){
  scatterplot <- ggplot(data=rs_req,
                        aes_string(x=i, y="resale_price")) + 
    geom_point() + geom_smooth(method = lm)
  scatterplot_list[[i]] <- scatterplot
}
```

```{r}
#| eval: false
ggarrange(plotlist = scatterplot_list, ncol = 4, nrow = 4)
```

Overall...

## Multiple Linear Regression Model

### Visualise relationships of independent variables

#### SET GEOMETRY AS NULL FIRST

::: panel-tabset
##### code

```{r}
#| eval: false
rs_req_nogeom <- st_set_geometry(rs_req, NULL) 
```

##### Glimpse

```{r}
#| eval: false
glimpse(rs_req_nogeom)
```
:::

#### PLOT A SCATTERPLOT MATRIX

```{r}
#| eval: false
corrplot(cor(rs_req_nogeom[, 2:17]), diag = FALSE, order = "AOE",
          tl.pos = "td", tl.cex = 0.8, method = "number", type = "upper")
```

### Hedonic Pricing Model Using Multiple Linear Regression Method

#### CALIBRATE THE MULTIPLE LINEAR REGRESSION MODEL

To calibrate the multiple linear regression model. And PROX_GOOD_PRISCH is excluded in this model.

```{r}
#| eval: false
rs_mlr1 <- lm(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, data=rs_req)

summary(rs_mlr1)
```

Results shows that

#### CALIBRATE THE REVISED MULTIPLE LINEAR REGRESSION MODEL

-   To calibrate the revised model
-   Using ols_regress() function to perform the Ordinary least squares regression (OLS)

```{r}
#| eval: false
rs_mlr1 <- lm(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL  + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, data=rs_req)

ols_regress(rs_mlr1)
```

SO...

### Check for multicolinearity

-   ols_vif_tol() function is used to check if there are any strong signs of multicollinearity.

```{r}
#| eval: false
ols_vif_tol(rs_mlr1)
```

It seems like...

### Test for Non-Linearity

```{r}
#| eval: false
ols_plot_resid_fit(rs_mlr1)
```

From the graph, ...

### Test for Normality Assumption

-   Since our sample size is more than 5000, we will not be using ols_test_normality() function for this analysis.
-   Instead, we will be using ols_plot_resid_hist() of olsrr package to perform normality assumption test.

```{r}
#| eval: false
ols_plot_resid_hist(rs_mlr1)
```

(must change accordingly) Results above reveals that the residual of the multiple linear regression model (i.e. rs_mlr1) resembles a normal distribution.

### Test for Spatial Autocorrelation

-   It is important to visualise the residual of the hedonic pricing model since the model is using geographically referenced attributes.

-   

    1.  Convert rs_req into a SpatialPointsDataFrame.

-   

    2.  Export the residual of the hedonic pricing model and save it as a data frame.

-   

    3.  Join the newly created data frame with rs_req object.

-   

    4.  Convert rs.res.sf simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects

#### EXPORT RESIDUAL OF HEDONIC PRICING MODEL

```{r}
#| eval: false
mlr.output <- as.data.frame(rs_mlr1$residuals)
```

#### JOIN WITH CONDO_RESALE.SF OBJECT

::: panel-tabset
##### code

```{r}
#| eval: false
rs.res.sf <- cbind(rs_req,
                   rs_mlr1$residuals) %>%
rename(`MLR_RES` = `rs_mlr1.residuals`)
```

##### Glimpse

```{r}
#| eval: false
glimpse(rs.res.sf)
```
:::

#### CONVERT TO SPATIALPOINTSDATAFRAME

```{r}
#| eval: false
rs.sp <- as_Spatial(rs.res.sf)
rs.sp
```

#### DISPLAY INTERACTIVE POINT SYMBOL MAP

To display the distribution of the residuals.

```{r}
#| eval: false
tmap_mode("view")
tm_basemap("OpenStreetMap")+
tm_shape(mpsz_sf)+
  tm_polygons(alpha = 0.4) +
tm_shape(rs.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

(need to change accordingly) From the plot above, we can see that there is somewhat spatial autocorrelation where similar values tend to be in similar location. However, to confirm that there truly is spatial autocorrelation, we will perform Moran's I test to validate our claims.

```{r}
#| eval: false
tmap_mode("plot")
```

## Moran's I test

### Obtaining upper distance band

```{r}
#| eval: false
coords <- coordinates(rs.sp)
k <- knn2nb(knearneigh(coords))
kdists <- unlist(nbdists(k, coords, longlat=FALSE))
summary(kdists) 
```

### Compute the distance-based weight matrix

```{r}
#| eval: false
nb <- dnearneigh(coordinates(rs.sp), 0, 1500, longlat = FALSE)
summary(nb)
```

### Convert to a spatial weights

-   nb2listw() of spdep package will be used to convert the output neighbours lists (i.e. nb) into a spatial weights

```{r}
#| eval: false
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```

### Perform Moran's I test for residual spatial autocorrelation

-   Use lm.morantest() of spdep package will be used to perform Moran's I test for residual spatial autocorrelation

```{r}
#| eval: false
lm.morantest(rs_mlr1, nb_lw)
```

(need to change) The Global Moran's I test for residual spatial autocorrelation shows that it's p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed. Since the Observed Global Moran I = 0.4111541 which is greater than 0, we can infer than the residuals resemble cluster distribution.

# Building Hedonic Pricing Models using GWmodel

## Build Fixed Bandwidth GWR Model

### Compute fixed bandwidth

-   bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.

There are 2 possible approaches to determine the stopping rule, they are: - CV cross-validation approach and - AIC corrected (AICc) approach.

We define the stopping rule using approach argument. For our analysis, since our sample size is quite huge, 15901 rows, we will not be using AIC as it works better for lower sample sizes.

```{r}
#| eval: false
# bw.fixed <- bw.gwr(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL  + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, data=rs.sp, approach="CV", kernel="gaussian", adaptive=FALSE, longlat=FALSE)
```

Due to the long run time, the output of the above computation will be shown in the following screenshot:

(insert img of result)

Results above show that:

(need to change accordingly) The CV score is becoming smaller and smaller. The recommended bandwidth is 570.6638 metres as it converged and stabilised here with the CV score of 1.068985e+13.

### GWModel method - fixed bandwidth

```{r}
#| eval: false
# gwr.fixed <- gwr.basic(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL  + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, data=rs.sp, bw=bw.fixed, kernel = 'gaussian', longlat = FALSE)
# gwr.fixed
```

Due to the long run time, the output of the above computation will be shown in the following screenshot:

(insert img of result)

Results above show that:

(need to change accordingly) The adjusted r-square of the GWR is 0.9572575 which is significantly better than the global multiple linear regression model of 0.7386.

## Build Adaptive Bandwidth GWR Model

In this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.

10.2.1 Compute the adaptive bandwidth Similar to the earlier section, we will first use bw.ger() to determine the recommended data point to use. The code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.

```{r}
#| eval: false
# bw.adaptive <- bw.gwr(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + 
#                         PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL  + PROX_SUPERMARKET  +
#                         WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, 
#                       data=rs.sp, approach="CV", kernel="gaussian", adaptive=TRUE, longlat=FALSE)
```

Due to the long run time, the output of the above computation will be shown in the following screenshot:

(img)

Results above show that:

### Construct the adaptive bandwidth GWR model

Calibrate the GWR-based hedonic pricing model by using adaptive bandwidth and gaussian kernel

```{r}
#| eval: false
# gwr.adaptive <- gwr.basic(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD +
#                             PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + 
#                             WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, 
#                           data=rs.sp, bw=bw.adaptive, kernel = 'gaussian', adaptive=TRUE, longlat = FALSE)
# gwr.adaptive
```

Due to the long run time, the output of the above computation will be shown in the following screenshot:

(img of result)

Results above show that:

(need to change accordingly) The adjusted r-square of the GWR is 0.9627267 which is significantly better than the global multiple linear regression model of 0.7386 Overall, we can see that the adaptive bandwidth GWR model has the best adjusted R-square value of 0.9627267 as compared to Multiple Linear Regression Model and Fixed Bandwidth GWR model. Hence, we will be using it to visualise GWR output in the next section.

# Visualising GWR Output

# Acknoledgement

-   Take-home Exercise 3 by NOR AISYAH BINTE AJIT.
-   In-class Exercise 9 by Prof Kam \[https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex09/in-class_ex09_gwml\]
