---
title: "Take-Home_Ex03"
date: "9 Mar 2023"
date-modified: "`r Sys.Date()`"
number-sections: true
format: html
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
---

# Overview

The price of housing is affected by many factors such as the general economy of a country or inflation rate. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

Conventional, housing resale prices predictive models were built by using [**Ordinary Least Square (OLS)**](https://en.wikipedia.org/wiki/Ordinary_least_squares) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, **Geographical Weighted Models** were introduced for calibrating predictive model for housing resale prices.

## Data Used

| Type                      | Name                              | Format  | Source                                                                                   |
|----------------|----------------|----------------|-------------------------|
| Aspatial                  | HDB Flat Resale Prices            | csv     | [data.gov](https://data.gov.sg/dataset/resale-flat-prices)                               |
| Geospatial                | Master Plan 2014 Subzone Boundary | shp     | [data.gov](https://data.gov.sg/dataset/master-plan-2014-subzone-boundary-web)            |
| Locational (w geo-coord)  | Eldercare services                | shp     | [data.gov](https://data.gov.sg/dataset/eldercare-services)                               |
| Locational (w geo-coord)  | Park facilities                   | geojson | [data.gov](https://data.gov.sg/dataset/park-facilities)                                  |
| Locational (w geo-coord)  | Hawker centres                    | geojson | [data.gov](https://data.gov.sg/dataset/hawker-centres)                                   |
| Locational (w geo-coord)  | Supermarkets                      | geojson | [data.gov](https://data.gov.sg/dataset/supermarkets)                                     |
| Locational (w geo-coord)  | Bus Stops                         | shp     | [Datamall.lta.gov.sg](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)  |
| Locational (w geo-coord)  | MRT                               | shp     | [Datamall.lta.gov.sg](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)  |
| Locational (w geo-coord)  | Childcare                         | csv     | [dataportal.asia](https://dataportal.asia/dataset/203030733_child-care-services)         |
| Locational (no geo-coord) | Kindergartens                     | csv     | [dataportal.asia](https://dataportal.asia/dataset/192512222_list-of-kindergartens)       |
| Locational (no geo-coord) | Primary school                    | csv     | [data.gov](https://data.gov.sg/dataset/school-directory-and-information)                 |
| Locational (no geo-coord) | CBD                               |         | Google search                                                                            |
| Locational (no geo-coord) | Shopping Malls                    | list    | [Wikipedia](https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore)           |
| Locational (no geo-coord) | Good primary school               | list    | [Local Salary Forum](https://www.salary.sg/2021/best-primary-schools-2021-by-popularity) |

## Packages

\[References taken from [Take-home Exercise 3](https://aisyahajit2018-is415.netlify.app/posts/2021-11-07-take-home-exercise-3/) by NOR AISYAH BINTE AJIT.\]

\[References taken from [Take-Home Exercise 3: Hedonic Pricing Models for Resale Prices of Public Housing in Singapore](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/) by MEGAN SIM TZE YEN.\]

-   **sf**: used for importing, managing, and processing geospatial data

-   **tidyverse**: a collection of packages for data science tasks

-   **tmap**: used for creating thematic maps, such as choropleth and bubble maps

-   **spdep**: used to create spatial weights matrix objects, global and local spatial autocorrelation statistics and related calculations (e.g. spatially lag attributes)

-   **httr**: used to make API calls, such as a GET request

-   **jsonlite**: a JSON parser that can convert from JSON to the appropraite R data types

-   **rvest: A new package that makes it easy to scrape (or harvest) data from html web pages, inspired by libraries like beautiful soup.**

    -   In this analysis, it will be used to scrape data for **Shopping malls** and **Good primary schools**

**Tidyverse packages:**

-   **readr** for importing delimited files (.csv)

-   **readxl** for importing Excel worksheets (.xlsx) - note that it has to be loaded explicitly as it is not a core tidyverse package

-   **tidyr** for manipulating and tidying data

-   **dplyr** for wrangling and transforming data

-   **ggplot2** for visualising data

**Building + visualising hedonic pricing models:**

-   **olsrr**: used for building least squares regression models

-   **coorplot** + **ggpubr**: both are used for multivariate data visualisation & analysis

-   **GWmodel**: provides a collection of localised spatial statistical methods, such as summary statistics, principal components analysis, discriminant analysis and various forms of GW regression

**Visualisations:**

-   **devtools:** used for installing any R packages which is not available in RCRAN. In this exercise, I will be installing using devtools to install the package xaringanExtra which is still under development stage.

-   **kableExtra**: an extension of kable, used for table customisation

-   **plotly**: used for creating interactive web graphics, and can be used in conjunction with ggplot2 with the `ggplotly()` function

-   **ggthemes**: an extension of ggplot2, with more advanced themes for plotting

## Import Packages

```{r}
pacman::p_load(sf, tidyverse, tmap, httr, rvest, spdep, readxl, jsonlite, olsrr, corrplot, ggpubr, GWmodel, kableExtra, plotly, ggthemes, broom, devtools, SpatialML, rsample, Metrics, ranger)
```

```{r}
#devtools::install_github("gadenbuie/xaringanExtra")
library(xaringanExtra)
```

# Aspatial Data Wrangling

\[References taken from [Take-home Exercise 3](https://aisyahajit2018-is415.netlify.app/posts/2021-11-07-take-home-exercise-3/) by NOR AISYAH BINTE AJIT.\]

```{r}
resale <- read_csv("data/aspatial/resale-flat-prices.csv")
glimpse(resale)
```

## Filter the data

The study should focus on either three-room, four-room or five-room flat and transaction period should be from 1st January 2021 to 31st December 2022. The test data should be January and February 2023 resale prices.

For this project, I will be going with 4 room flats.

::: panel-tabset
#### Code

```{r}
rs_subset <-  filter(resale,flat_type == "4 ROOM") %>%
                    filter(month >= "2021-01" & month <= "2023-02")
```

#### Glimpse

```{r}
glimpse(rs_subset)
```

#### Unique month

```{r}
unique(rs_subset$month)
```

#### Unique flat_type

```{r}
unique(rs_subset$flat_type)
```
:::

## Transform resale data

After checking the correctly filtered out data, next is transforming the data,

### New columns

::: panel-tabset
#### Code

```{r}
rs_transform <- rs_subset %>%
  mutate(rs_subset, address = paste(block,street_name)) %>%
  mutate(rs_subset, remaining_lease_yr = as.integer(str_sub(remaining_lease, 0, 2))) %>%
  mutate(rs_subset, remaining_lease_mth = as.integer(str_sub(remaining_lease, 9, 11)))
```

#### Head

```{r}
head(rs_transform)
```
:::

### Add up the remaining lease in months

-   Replace NA values in remaining_lease_mth with the value 0 using is.na() function
-   Multiply remaining_lease_yr by 12 to convert it to months unit
-   Create remaining_lease_mths column using mutate function of dplyr package which contains the summation of the remaining_lease_yr and remaining_lease_mths using rowSums() function of base R package
-   Select required columns for analysis

::: panel-tabset
#### Code

```{r}
rs_transform$remaining_lease_mth[is.na(rs_transform$remaining_lease_mth)] <- 0
rs_transform$remaining_lease_yr <- rs_transform$remaining_lease_yr * 12
rs_transform <- rs_transform %>% 
  mutate(rs_transform, remaining_lease_mths = rowSums(rs_transform[, c("remaining_lease_yr", "remaining_lease_mth")])) %>%
  select(month, town, address, block, street_name, flat_type, storey_range, floor_area_sqm, flat_model, 
         lease_commence_date, remaining_lease_mths, resale_price)
```

#### Head

```{r}
head(rs_transform)
```
:::

## Retrieve Postal Codes and Coordinates of Addresses

This section aims to get the postal codes and coordinates which is needed for the locational factors without geographical coordinates.

### Create a list storing unique addresses

-   Use unique() function to extract the unique addresses then use sort() function to sort the unique vector.

```{r}
#|eval: false
add_list <- sort(unique(rs_transform$address))
```

## Create function to retrieve coordinates from OneMap.Sg API

```{r}
#| eval: false
get_coords <- function(add_list){
  
  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (i in add_list){
    #print(i)

    r <- GET('https://developers.onemap.sg/commonapi/search?',
           query=list(searchVal=i,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
    }
    
    # If multiple results, drop NIL and append top 1
    else if (found > 1){
      # Remove those with NIL as postal
      res_sub <- res[res$POSTAL != "NIL", ]
      
      # Set as NA first if no Postal
      if (nrow(res_sub) == 0) {
          new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
      }
      
      else{
        top1 <- head(res_sub, n = 1)
        postal <- top1$POSTAL 
        lat <- top1$LATITUDE
        lng <- top1$LONGITUDE
        new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
      }
    }

    else {
      new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  return(postal_coords)
}
```

### Call get_coords function to retrieve resale coordinates

```{r}
#| eval: false
coords <- get_coords(add_list)
```

### Inspect results

```{r}
#| eval: false
coords[(is.na(coords$postal) | is.na(coords$latitude) | is.na(coords$longitude) | coords$postal=="NIL"), ]
```

It seems like there is 1 address that does not contain geographic coordinates. After further investigation, I will be input in

```{r}
#| eval: false
coords$postal[coords$postal=="NIL"] <- "680215"
```

```{r}
#| eval: false
coords[(is.na(coords$postal) | is.na(coords$latitude) | is.na(coords$longitude) | coords$postal=="NIL"), ]
```

great, no more NIL postal codes.

### Combine resale and coordinates data

Now, we need to combine the data

::: panel-tabset
#### Code

```{r}
#| eval: false
rs_coords <- left_join(rs_transform, coords, by = c('address' = 'address'))
```

#### Head

```{r}
#| eval: false
head(rs_coords)
```
:::

### Handle invalid addresses

By replacing sub string in invalid addresses in the address and extract rows with addresses containing SAINT GEORGE'S

::: panel-tabset
#### Code

```{r}
#| eval: false
rs_coords$address <- sub("ST. GEORGE'S", "SAINT GEORGE'S", rs_coords$address)
rs_invalid <- rs_coords[grepl("SAINT GEORGE'S", rs_coords$address), ]
```

#### Glimpse

```{r}
#| eval: false
glimpse(rs_invalid)
```
:::

There are 32 rows that contains SAINT GEORGE'S as street name but has a substring replaced in the address.

#### Create unique list of addresses again

```{r}
#| eval: false
add_list <- sort(unique(rs_invalid$address))
```

#### Call get_coords to retrieve resale coordinates again

```{r}
#| eval: false
rs_invalid_coords <- get_coords(add_list)
```

#### Inspect results again

```{r}
#| eval: false
rs_invalid_coords[(is.na(rs_invalid_coords$postal) | is.na(rs_invalid_coords$latitude) | is.na(rs_invalid_coords$longitude)), ]
```

So the results shows no invalid coordinates now.

#### Combine rs_invalid_coords with rs_coords data

::: panel-tabset
##### Code

```{r}
#| eval: false
rs_coords_final <- rs_coords %>%
  left_join(rs_invalid_coords, by = c("address")) %>%
  mutate(latitude = ifelse(is.na(postal.x), postal.y, postal.x)) %>%
  mutate(latitude = ifelse(is.na(latitude.x), latitude.y, latitude.x)) %>%
  mutate(longitude = ifelse(is.na(longitude.x), longitude.y, longitude.x)) %>%
  select(-c(postal.x, latitude.x, longitude.x, postal.y, latitude.y, longitude.y))
```

##### Head

```{r}
#| eval: false
head(rs_coords_final)
```
:::

## Write file to rds

```{r}
#| eval: false
rs_coords_rds <- write_rds(rs_coords_final, "data/aspatial/rds/rs_coords.rds")
```

## Read rs_coords RDS file

::: panel-tabset
#### Code

```{r}
rs_coords <- read_rds("data/aspatial/rds/rs_coords.rds")
```

#### Glimpse

```{r}
glimpse(rs_coords)
```
:::

### Assign and Transform CRS and Check

::: panel-tabset
#### Code

```{r}
rs_coords_sf <- st_as_sf(rs_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

#### st_crs

```{r}
st_crs(rs_coords_sf)
```
:::

### Check for invalid geometries

```{r}
length(which(st_is_valid(rs_coords_sf) == FALSE))
```

No invalid geometries

### Plot hdb resale points

```{r}
tmap_mode("view")
tm_shape(rs_coords_sf)+
  tm_dots(col="blue", size = 0.02)
  tm_view(set.zoom.limits = c(6,8))
tmap_mode("plot")
```

# Locational with geographical coordinates

## Read and check CRS of Locational factors

::: panel-tabset
#### elder_sf

```{r}
elder_sf <- st_read(dsn = "data/geospatial/eldercare-services", layer = "ELDERCARE")
```

##### st_crs

```{r}
st_crs(elder_sf)
```

#### parks_sf

```{r}
parks_sf <- st_read("data/geospatial/park-facilities/park-facilities-geojson.geojson") 
```

##### st_crs

```{r}
st_crs(parks_sf)
```

#### hawker_sf

```{r}
hawker_sf <- st_read("data/geospatial/hawker-centres/hawker-centres-geojson.geojson") 
```

##### st_crs

```{r}
st_crs(hawker_sf)
```

#### supermkt_sf

```{r}
supermkt_sf <- st_read("data/geospatial/supermarkets/supermarkets-geojson.geojson")
```

##### st_crs

```{r}
st_crs(supermkt_sf)
```

#### childcare_sf

```{r}
childcare_sf <- st_read("data/geospatial/childcare/childcare.geojson")
```

##### st_crs

```{r}
st_crs(childcare_sf)
```

#### kind_sf

```{r}
kind_sf <- st_read("data/geospatial/kindergartens/preschools-location.geojson")
```

##### st_crs

```{r}
st_crs(kind_sf)
```

#### mrt_sf

```{r}
mrtlrt_sf <- st_read(dsn = "data/geospatial/TrainStation", layer="Train_Station_Exit_Layer")
```

##### st_crs

```{r}
st_crs(mrtlrt_sf)
```

#### bus_sf

```{r}
bus_sf <- st_read(dsn = "data/geospatial/BusStopLocation", layer="BusStop")
```

##### st_crs

```{r}
st_crs(bus_sf)
```
:::

### Assign EPSG code to sf dataframes and check again

::: panel-tabset
#### Code

```{r}
elder_sf <- st_set_crs(elder_sf, 3414)
mrtlrt_sf <- st_set_crs(mrtlrt_sf, 3414)
bus_sf <- st_set_crs(bus_sf, 3414)

hawker_sf <- hawker_sf %>%
  st_transform(crs = 3414)
parks_sf <- parks_sf %>%
  st_transform(crs = 3414)
supermkt_sf <- supermkt_sf %>%
  st_transform(crs = 3414)
childcare_sf <- childcare_sf %>%
  st_transform(crs = 3414)
kind_sf <- kind_sf %>%
  st_transform(crs = 3414)
```

#### st_crs

```{r}
st_crs(elder_sf)
st_crs(mrtlrt_sf)
st_crs(bus_sf)
st_crs(hawker_sf)
st_crs(parks_sf)
st_crs(supermkt_sf)
st_crs(childcare_sf)
st_crs(kind_sf)
```
:::

## Check for invalid geometries

```{r}
length(which(st_is_valid(elder_sf) == FALSE))
length(which(st_is_valid(mrtlrt_sf) == FALSE))
length(which(st_is_valid(hawker_sf) == FALSE))
length(which(st_is_valid(parks_sf) == FALSE))
length(which(st_is_valid(supermkt_sf) == FALSE))
length(which(st_is_valid(childcare_sf) == FALSE))
length(which(st_is_valid(kind_sf) == FALSE))
length(which(st_is_valid(bus_sf) == FALSE))
```

NO invalid geometries.

## Calculate Proximity

```{r}
#| eval: false
get_prox <- function(origin_df, dest_df, col_name){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)           
  
  # find the nearest location_factor and create new data frame
  near <- origin_df %>% 
    mutate(PROX = apply(dist_matrix, 1, function(x) min(x)) / 1000) 
  
  # rename column name according to input parameter
  names(near)[names(near) == 'PROX'] <- col_name

  # Return df
  return(near)
}
```

#### CALL GET_PROX FUNCTION

```{r}
#| eval: false
rs_coords_sf <- get_prox(rs_coords_sf, elder_sf, "PROX_ELDERLYCARE")
rs_coords_sf <- get_prox(rs_coords_sf, mrtlrt_sf, "PROX_MRT")
rs_coords_sf <- get_prox(rs_coords_sf, hawker_sf, "PROX_HAWKER")
rs_coords_sf <- get_prox(rs_coords_sf, parks_sf, "PROX_PARK")
rs_coords_sf <- get_prox(rs_coords_sf, supermkt_sf, "PROX_SUPERMARKET")
```

## Create get_within function to calculate no. of factors within dist

```{r}
#| eval: false
get_within <- function(origin_df, dest_df, threshold_dist, col_name){

  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)

  # count the number of location_factors within threshold_dist and create new data frame
  wdist <- origin_df %>%
    mutate(WITHIN_DT = apply(dist_matrix, 1, function(x) sum(x <= threshold_dist)))

  # rename column name according to input parameter
  names(wdist)[names(wdist) == 'WITHIN_DT'] <- col_name

  # Return df
  return(wdist)
}
```

#### CALL GET_WITHIN FUNCTION

The threshold will be set to 350m for locational factors such as, Kindergartens, Childcare centres and Bus stops.

##### kindergarten

::: panel-tabset
###### Code

```{r}
#| eval: false
rs_coords_sf <- get_within(rs_coords_sf, kind_sf, 350, "WITHIN_350M_KINDERGARTEN")
```

###### Head

```{r}
#| eval: false
head(rs_coords_sf)
```
:::

##### Childcare centres

::: panel-tabset
###### Code

```{r}
#| eval: false
rs_coords_sf <- get_within(rs_coords_sf, childcare_sf, 350, "WITHIN_350M_CHILDCARE")
```

###### Head

```{r}
#| eval: false
head(rs_coords_sf)
```
:::

##### Bus stops

::: panel-tabset
###### Code

```{r}
#| eval: false
rs_coords_sf <- get_within(rs_coords_sf, bus_sf, 350, "WITHIN_350M_BUS")
```

###### Head

```{r}
#| eval: false
head(rs_coords_sf)
```
:::

# Locational factors without geographic coordinates

## CBD

according to google, the latitude and longitude of Downtown Core (aka CBD), are 1.287953 and 103.851784 respectively.

#### STORE CBD COORDINATES IN DATAFRAME

```{r}
#| eval: false
name <- c('CBD Area')
latitude= c(1.287953)
longitude= c(103.851784)
cbd_coords <- data.frame(name, latitude, longitude)
```

#### ASSIGN AND TRANSFORM CRS

To convert the data frame into sf object then, transform the coordinates of the sf object

::: panel-tabset
##### code

```{r}
#| eval: false
cbd_coords_sf <- st_as_sf(cbd_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) |>
  st_transform(crs = 3414)
```

##### st_crs

```{r}
#| eval: false
st_crs(cbd_coords_sf)
```
:::

The coordinates for CBD area is in EPSG 3414 (SVY21) format is c(30055.05, 30040.83).

#### CALL GET_PROX FUNCTION

Get the proximity of HDB and CBD area.

```{r}
#| eval: false
rs_coords_sf <- get_prox(rs_coords_sf, cbd_coords_sf, "PROX_CBD") 
```

## Shopping mall

In a nutshell, the following code chunk will perform 3 steps: - Read the Wikipedia html page containing the Shopping Malls in Singapore - Read the text portion (html_text()) of the Unordered - - List element selected by html_nodes() - Append it to the empty mall_list created \#### EXTRACT SHOPPING MALLS FROM WIKIPEDIA

::: panel-tabset
##### Code

```{r}
#| eval: false
url <- "https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore"
malls_list <- list()

for (i in 2:7){
  malls <- read_html(url) %>%
    html_nodes(xpath = paste('//*[@id="mw-content-text"]/div[1]/div[',as.character(i),']/ul/li',sep="") ) %>%
    html_text()
  malls_list <- append(malls_list, malls)
}
```

##### st_crs

```{r}
#| eval: false
st_crs(malls_list)
```
:::

#### CALL GET_COORDS FUNCTION

To retrieve coordinates of Shopping Malls and rename the address to mall_name for easier reference

```{r}
#| eval: false
malls_list_coords <- get_coords(malls_list) %>% 
  rename("mall_name" = "address")
```

#### REMOVE INVALID SHOPPING MALL NAME

```{r}
#| eval: false
malls_list_coords <- subset(malls_list_coords, mall_name!= "Yew Tee Shopping Centre")
```

#### CORRECT INVALID MALL NAMES THAT CAN BE FOUND

```{r}
#| eval: false
invalid_malls<- subset(malls_list_coords, is.na(malls_list_coords$postal))
invalid_malls_list <- unique(invalid_malls$mall_name)
corrected_malls <- c("Clarke Quay", "City Gate", "Raffles Holland V", "Knightsbridge", "Mustafa Centre", "GR.ID", "Shaw House",
                     "The Poiz Centre", "Velocity @ Novena Square", "Singapore Post Centre", "PLQ Mall", "KINEX", "The Grandstand")

for (i in 1:length(invalid_malls_list)) {
  malls_list_coords <- malls_list_coords %>% 
    mutate(mall_name = ifelse(as.character(mall_name) == invalid_malls_list[i], corrected_malls[i], as.character(mall_name)))
}
```

#### CREATE A LIST STORING UNIQUE MALL NAMES

```{r}
#| eval: false
malls_list <- sort(unique(malls_list_coords$mall_name))
```

#### CALL GET_COORDS TO RETRIEVE COORDINATES OF SHOPPING MALLS AGAIN

```{r}
#| eval: false
malls_coords <- get_coords(malls_list)
```

#### INSPECT RESULTS

```{r}
#| eval: false
malls_coords[(is.na(malls_coords$postal) | is.na(malls_coords$latitude) | is.na(malls_coords$longitude)), ]
```

#### CONVERT DATA FRAME INTO SF OBJECT, ASSIGN AND TRANSFORM CRS

```{r}
#| eval: false
malls_sf <- st_as_sf(malls_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

#### CALL GET_PROX FUNCTION

```{r}
#| eval: false
rs_coords_sf <- get_prox(rs_coords_sf, malls_sf, "PROX_MALL") 
```

## Primary Schools

### READ IN CSV FILE

::: panel-tabset
#### code

```{r}
pri_sch <- read_csv("data/geospatial/prisch/general-information-of-schools.csv")
```

#### glimpse

```{r}
glimpse(pri_sch)
```
:::

### EXTRACT PRIMARY SCHOOLS AND REQUIRED COLUMNS ONLY

::: panel-tabset
#### code

```{r}
#| eval: false
pri_sch <- pri_sch %>%
  filter(mainlevel_code == "PRIMARY") %>%
  select(school_name, address, postal_code, mainlevel_code)
```

#### glimpse

```{r}
#| eval: false
glimpse(pri_sch)
```
:::

#### CREATE LIST STORING UNIQUE POSTAL CODES OF PRIMARY SCHOOLS

```{r}
#| eval: false
prisch_list <- sort(unique(pri_sch$postal_code))
```

#### CALL GET_COORDS FUNCTION TO RETRIEVE COORDINATES OF PRIMARY SCHOOLS

```{r}
#| eval: false
prisch_coords <- get_coords(prisch_list)
```

#### INSPECT RESULTS

```{r}
#| eval: false
prisch_coords[(is.na(prisch_coords$postal) | is.na(prisch_coords$latitude) | is.na(prisch_coords$longitude)), ]
```

No NA values.

#### COMBINE COORDINATES WITH PRIMARY SCHOOL NAMES

To verify whether we have extracted it correctly.

::: panel-tabset
##### code

```{r}
#| eval: false
prisch_coords = prisch_coords[c("postal","latitude", "longitude")]
pri_sch <- left_join(pri_sch, prisch_coords, by = c('postal_code' = 'postal'))
```

##### head

```{r}
#| eval: false
head(pri_sch)
```
:::

#### CONVERT PRI_SCH DATA FRAME INTO SF OBJECT, ASSIGN AND TRANSFORM CRS

::: panel-tabset
##### code

```{r}
#| eval: false
prisch_sf <- st_as_sf(pri_sch,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

##### st_crs

```{r}
#| eval: false
st_crs(prisch_sf)
```
:::

#### CALL GET_WITHIN FUNCTION

::: panel-tabset
##### code

```{r}
#| eval: false
rs_coords_sf <- get_within(rs_coords_sf, prisch_sf, 1000, "WITHIN_1KM_PRISCH")
```

##### head

```{r}
#| eval: false
head(rs_coords_sf)
```
:::

## Good primary school

### EXTRACT RANKING LIST OF PRIMARY SCHOOLS

::: panel-tabset
#### code

```{r}
#| eval: false
url <- "https://www.salary.sg/2021/best-primary-schools-2021-by-popularity/"

good_pri <- data.frame()

schools <- read_html(url) %>%
  html_nodes(xpath = paste('//*[@id="post-3068"]/div[3]/div/div/ol/li') ) %>%
  html_text() 

for (i in (schools)){
  sch_name <- toupper(gsub(" – .*","",i))
  sch_name <- gsub("\\(PRIMARY SECTION)","",sch_name)
  sch_name <- trimws(sch_name)
  new_row <- data.frame(pri_sch_name=sch_name)
  # Add the row
  good_pri <- rbind(good_pri, new_row)
}

top_good_pri <- head(good_pri, 10)
```

#### head

```{r}
#| eval: false
head(top_good_pri)
```
:::

#### CHECK FOR GOOD PRIMARY SCHOOLS IN PRIMARY SCHOOL DF

To check whether the names of the good primary schools is similar to the names in primary school dataframe

```{r}
#| eval: false
top_good_pri$pri_sch_name[!top_good_pri$pri_sch_name %in% prisch_sf$school_name]
```

#### CREATE A LIST STORING UNIQUE GOOD PRIMARY SCHOOL NAMES

```{r}
#| eval: false
good_pri_list <- unique(top_good_pri$pri_sch_name)
```

#### CALL GET_COORDS FUNCTION TO RETRIEVE COORDINATES OF GOOD PRIMARY SCHOOLS

```{r}
#| eval: false
goodprisch_coords <- get_coords(good_pri_list)
```

#### INSPECT RESULTS

```{r}
#| eval: false
goodprisch_coords[(is.na(goodprisch_coords$postal) | is.na(goodprisch_coords$latitude) | is.na(goodprisch_coords$longitude)), ]
```

#### REPLACE INVALID GOOD PRIMARY SCHOOL NAMES

```{r}
#| eval: false
top_good_pri$pri_sch_name[top_good_pri$pri_sch_name == "CHIJ ST. NICHOLAS GIRLS’ SCHOOL"] <- "CHIJ SAINT NICHOLAS GIRLS' SCHOOL"
top_good_pri$pri_sch_name[top_good_pri$pri_sch_name == "ST. HILDA’S PRIMARY SCHOOL"] <- "SAINT HILDA'S PRIMARY SCHOOL"
```

#### CREATE A LIST STORING UNIQUE GOOD PRIMARY SCHOOL NAMES AGAIN

```{r}
#| eval: false
good_pri_list <- unique(top_good_pri$pri_sch_name)
```

#### CALL GET_COORDS FUNCTION TO RETRIEVE COORDINATES OF GOOD PRIMARY SCHOOLS AGAIN

```{r}
#| eval: false
goodprisch_coords <- get_coords(good_pri_list)
```

#### INSPECT RESULTS AGAIN

```{r}
#| eval: false
goodprisch_coords[(is.na(goodprisch_coords$postal) | is.na(goodprisch_coords$latitude) | is.na(goodprisch_coords$longitude)), ]
```

#### CONVERT DATA FRAME INTO SF OBJECTS, ASSIGN AND TRANSFORM CRS

::: panel-tabset
##### code

```{r}
#| eval: false
goodpri_sf <- st_as_sf(goodprisch_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

##### st_crs

```{r}
#| eval: false
st_crs(goodpri_sf)
```
:::

#### CALL GET_PROX FUNCTION

```{r}
#| eval: false
rs_coords_sf <- get_prox(rs_coords_sf, goodpri_sf, "PROX_GOOD_PRISCH")
```

## Write to RDS file

To prevent running of the codes

```{r}
#| eval: false
rs_factors_rds <- write_rds(rs_coords_sf, "data/aspatial/rds/rs_factors.rds")
```

# Import Data for Analysis

## Geospatial data

### MP2019

::: panel-tabset
#### Code

```{r}
mpsz_sf <- st_read(dsn = "data/geospatial/mpsz", layer="MPSZ-2019")
```

#### st_crs()

```{r}
st_crs(mpsz_sf)
```
:::

### Transform CRS

::: panel-tabset
#### Code

```{r}
mpsz_sf <- st_transform(mpsz_sf, 3414)
```

#### st_crs()

```{r}
st_crs(mpsz_sf)
```
:::

### Remove invalid geometries (if any)

#### CHECK FOR INVALID GEOMETRIES

```{r}
length(which(st_is_valid(mpsz_sf) == FALSE))
```

#### HANDLE INVALID GEOMETRIES AND CHECK

```{r}
mpsz_sf <- st_make_valid(mpsz_sf)
length(which(st_is_valid(mpsz_sf) == FALSE))
```

### Reveal the extent of mpsz_sf

```{r}
st_bbox(mpsz_sf)
```

# Data preparation - Resale with locational factors

## Read RDS file

::: panel-tabset
#### Code

```{r}
rs_sf <- read_rds("data/aspatial/rds/rs_factors.rds")
```

#### Glimpse

```{r}
glimpse(rs_sf)
```
:::

### Extract unique storey_range and sort

```{r}
storeys <- sort(unique(rs_sf$storey_range))
```

### Create dataframe storey_range_order to store order of storey_range

::: panel-tabset
#### Code

```{r}
storey_order <- 1:length(storeys)
storey_range_order <- data.frame(storeys, storey_order)
```

#### Head

```{r}
head(storey_range_order)
```
:::

### Combine storey_order with resale dataframe

::: panel-tabset
#### Code

```{r}
rs_sf <- left_join(rs_sf, storey_range_order, by= c("storey_range" = "storeys"))
```

#### Glimpse

```{r}
glimpse(rs_sf)
```
:::

### Select required columns for analysis

::: panel-tabset
#### Code

```{r}
rs_req <- rs_sf %>%
  select(month, resale_price, floor_area_sqm, storey_order, remaining_lease_mths,
         PROX_CBD, PROX_ELDERLYCARE, PROX_HAWKER, PROX_MRT, PROX_PARK, PROX_GOOD_PRISCH, PROX_MALL,
         PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, WITHIN_350M_BUS, WITHIN_1KM_PRISCH)
```

#### Glimpse

```{r}
glimpse(rs_req)
```
:::

### View summary

```{r}
summary(rs_req)
```

## statistical graphics

### Plot Histogram of resale_price

```{r}
ggplot(data=rs_req, aes(x=`resale_price`)) +
  geom_histogram(bins=20, color="black", fill="light coral")
```

### Normalise using Log Transformation

```{r}
rs_req <- rs_req %>%
  mutate(`LOG_SELLING_PRICE` = log(resale_price))
```

### Plot Histogram of LOG_RESALE_PRICE

```{r}
ggplot(data=rs_req, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light green")
```

# Exploratory Data Analysis (EDA)

## Multiple Histogram Plots distribution of variables

### Stuctural Factors

#### EXTRACT COLUMN NAMES TO PLOT

```{r}
s_factor <- c("floor_area_sqm", "storey_order", "remaining_lease_mths")
```

#### CREATE A LIST TO STORE HISTOGRAMS OF STUCTURAL FACTORS

```{r}
s_factor_hist_list <- vector(mode = "list", length = length(s_factor))
for (i in 1:length(s_factor)) {
  hist_plot <- ggplot(rs_req, aes_string(x = s_factor[[i]])) +
    geom_histogram(color="firebrick", fill = "light coral") +
    labs(title = s_factor[[i]]) +
    theme(plot.title = element_text(size = 10),
          axis.title = element_blank())
  
  s_factor_hist_list[[i]] <- hist_plot
}
```

#### PLOT HISTOGRAMS TO EXAMINE DISTRIBUTION OF STUCTURAL FACTORS

```{r}
ggarrange(plotlist = s_factor_hist_list,
          ncol = 2,
          nrow = 2)
```

### Locational Factors

#### EXTRACT COLUMN NAMES TO PLOT

```{r}
l_factor <- c("PROX_CBD", "PROX_ELDERLYCARE", "PROX_HAWKER", "PROX_MRT", "PROX_PARK", "PROX_GOOD_PRISCH", "PROX_MALL",
              "PROX_SUPERMARKET", "WITHIN_350M_KINDERGARTEN", "WITHIN_350M_CHILDCARE", "WITHIN_350M_BUS", "WITHIN_1KM_PRISCH")
```

#### CREATE A LIST TO STORE HISTOGRAMS OF LOCATIONAL FACTORS

```{r}
l_factor_hist_list <- vector(mode = "list", length = length(l_factor))
for (i in 1:length(l_factor)) {
  hist_plot <- ggplot(rs_req, aes_string(x = l_factor[[i]])) +
    geom_histogram(color="midnight blue", fill = "light sky blue") +
    labs(title = l_factor[[i]]) +
    theme(plot.title = element_text(size = 10),
          axis.title = element_blank())
  
  l_factor_hist_list[[i]] <- hist_plot
}
```

#### PLOT HISTOGRAMS TO EXAMINE DISTRIBUTION OF LOCATIONAL FACTORS

```{r}
ggarrange(plotlist = l_factor_hist_list,
          ncol = 4,
          nrow = 4)
```

## Statistical Point Map

```{r}
tmap_mode("view")
tm_shape(rs_sf) +  
  tm_dots(col = "resale_price",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14)) +
tm_basemap("OpenStreetMap")

```

```{r}
tmap_mode("plot")
```

# Train & Test Data

### Split train and test data sets

-   Train data - Jan 2021 to Dec 2022
-   Test data - Jan 2023 to feb 2023

```{r}
#| eval: false
train_data <- rs_req |>
  filter(rs_req$month >= '2021-01' & rs_req$month <= '2022-12')

test_data <- rs_req |>
  filter(rs_req$month >= '2023-01' & rs_req$month <= '2023-02')
```

### write train and test to rds files

```{r}
#| eval: false
write_rds(train_data, "data/aspatial/rds/train_data.rds")
write_rds(test_data, "data/aspatial/rds/test_data.rds")
```

```{r}
#| eval: false
rs_req_nogeo <- st_set_geometry(rs_req, NULL) 
```

```{r}
#| eval: false
glimpse(rs_req_nogeo)
```

## Computing Correlation Matrix

```{r}
rs_req_nogeo <- rs_req %>%
  st_drop_geometry()
corrplot::corrplot(cor(rs_req_nogeo[, 2:17]), 
                   diag = FALSE, 
                   order = "AOE",
                   tl.pos = "td", 
                   tl.cex = 0.8, 
                   method = "number", 
                   type = "upper")

```

## Retriving the Stored Data

```{r}
train_data <- read_rds("data/aspatial/rds/train_data.rds")
test_data <- read_rds("data/aspatial/rds/test_data.rds")
```

## Building a non-spatial multiple linear regression

```{r}
#| eval: false
price_mlr <- lm(resale_price ~ floor_area_sqm +
                  storey_order + remaining_lease_mths +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                data=train_data)
summary(price_mlr)
```

## write price_mlr to rds file

```{r}
#| eval: false
write_rds(price_mlr, "data/aspatial/rds/price_mlr.rds" ) 
```

# gwr predictive method

In this section, you will learn how to calibrate a model to predict HDB resale price by using geographically weighted regression method of GWmodel package.

## Converting the sf data.frame to SpatialPointDataFrame

```{r}
#| eval: false
train_data_sp <- as_Spatial(train_data)
train_data_sp
```

## Computing adaptive bandwidth

Next, bw.gwr() of GWmodel package will be used to determine the optimal bandwidth to be used.

-   To determine adaptive bandwidth and CV method is used to determine the optimal bandwidth.

```{r}
#| eval: false
bw_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +
                  storey_order + remaining_lease_mths +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                  data=train_data_sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)
```

![](data/bw_adaptive.JPG)


Results above show that:
-   118 is the recommended data points to be used

## write bw_adpative to rds file

```{r}
#| eval: false
write_rds(bw_adaptive, "data/aspatial/rds/bw_adaptive.rds")
```

```{r}
bw_adaptive <- read_rds("data/aspatial/rds/bw_adaptive.rds")
```

## Calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel

```{r}
#| eval: false
gwr_adaptive <- gwr.basic(formula = resale_price ~
                            floor_area_sqm + storey_order +
                            remaining_lease_mths + PROX_CBD + 
                            PROX_ELDERLYCARE + PROX_HAWKER +
                            PROX_MRT + PROX_PARK + PROX_MALL + 
                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                            WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                            WITHIN_1KM_PRISCH,
                          data=train_data_sp,
                          bw=bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE,
                          longlat = FALSE)
```

![](data/gwr_adaptive_part1.JPG)

![](data/gwr_adaptive_part2.JPG)

![](data/gwr_adaptive_part3.JPG){width="344"}

![](data/gwr_adaptive_part4.JPG){width="467"}

![](data/gwr_adaptive_part5.JPG)

![](data/gwr_adaptive_part6.JPG){width="340"}

![](data/gwr_adaptive_part7.JPG)

Overall the values between the dependent and independent variables are similar in range.

### save to rds

```{r}
#| eval: false
write_rds(gwr_adaptive, "data/aspatial/rds/gwr_adaptive.rds")
```

## Output

```{r}
#| eval: false
gwr_adaptive <- read_rds("data/aspatial/rds/gwr_adaptive.rds")
gwr_adaptive
```

# Preparing coordinates data

## Extracting coordinates data

extract the x,y coordinates of the full, training and test data sets.

```{r}
coords_train <- st_coordinates(train_data)
coords_test <- st_coordinates(test_data)
```

### write all the output into rds for future used

```{r}

coords_train <- write_rds(coords_train, "data/aspatial/rds/coords_train.rds" )
coords_test <- write_rds(coords_test, "data/aspatial/rds/coords_test.rds" )
```

## Droping geometry field

```{r}

train_data <- train_data %>% 
  st_drop_geometry()
```

# Calibrating Random Forest Model

In this section, we will be calibrating a model to predict HDB resale price by using random forest function of ranger package.

```{r}
#| eval: false
set.seed(1234)
rf <- ranger(resale_price ~ floor_area_sqm + storey_order + 
               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + 
               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + 
               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + 
               WITHIN_1KM_PRISCH,
             data=train_data)
```

```{r}
#| eval: false
print(rf)
```

![](data/rf.JPG)

## Calibrating Geographical Random Forest Model

In this section, we will be calibrating a model to predict HDB resale price by using grf() of SpatialML package.

### Calibrating using training data

Calibrate a geographic ranform forest model by using grf() of SpatialML package.

```{r}
#| eval: false
set.seed(1234)
gwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,
                     dframe=train_data, 
                     bw=55,
                     kernel="adaptive",
                     coords=coords_train)
```

## write to rds file

```{r}
#| eval: false
write_rds(gwRF_adaptive, "data/aspatial/gwRF_adaptive.rds")
```

## retrieve for future use

```{r}
#| eval: false
gwRF_adaptive <- read_rds("data/aspatial/gwRF_adaptive.rds")
```

## Predicting by using test data

#### prepare data

```{r}
#| eval: false
test_data <- cbind(test_data, coords_test) %>%
  st_drop_geometry()
```

Next, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.

```{r}
#| eval: false
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

### write to rds file

```{r}
#| eval: false
GRF_pred <- write_rds(gwRF_pred, "data/model/GRF_pred.rds")
```

## Converting the predicting output into a data frame

The output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.

```{r}
#| eval: false
GRF_pred <- read_rds("data/model/GRF_pred.rds")
GRF_pred_df <- as.data.frame(GRF_pred)
```

cbind() is used to append the predicted values onto test_data

```{r}
#| eval: false
test_data_p <- cbind(test_data, GRF_pred_df)
```

```{r}
#| eval: false
write_rds(test_data_p, "data/model/test_data_p.rds")
```

# Calculating Root Mean Square Error

The root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, rmse() of Metrics package is used to compute the RMSE.

```{r}
#| eval: false
rmse(test_data_p$resale_price, 
     test_data_p$GRF_pred)
```

## Visualising the predicted values

A better predictive model should have the scatter point close to the diagonal line. The scatter plot can be also used to detect if any outliers in the model.

```{r}
#| eval: false
ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = resale_price)) +
  geom_point()
```

# Ordinary Least Square method (OLS)

```{r}
rs_mlr1 <- lm(formula = resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL  + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH, data=rs_req)

ols_regress(rs_mlr1)
```

The adjusted r-square of the GWR is 0.715 which is worse than the GWR model of 0.7207. 

Overall, we can see that the adaptive bandwidth GWR model has the best adjusted R-square value of 0.9627267 as compared to Multiple Linear Regression Model and Fixed Bandwidth GWR model. Hence, we will be using it to visualise GWR output in the next section.

### **Check for multicolinearity**

```{r}
ols_vif_tol(rs_mlr1)
```

Since all the VIF values of the independent variables are less than 10, we can conclude that there is no signs of multicollinearity among the independent variables.

### **Test for Non-Linearity**

```{r}
ols_plot_resid_fit(rs_mlr1)
```
Most of the data points are scattered around the 0 line.
Thus, we can conclude that the relationships between the dependent variable and independent variables are linear.

### **Test for Normality Assumption**

```{r}
ols_plot_resid_hist(rs_mlr1)
```

Results above reveals that the residual of the multiple linear regression model seems like a normal distribution.

# Comparsion of OLS and GWR

The performance of GWR seems to be better than OLS since it can analyse the spatially varying relationships and capture spatial patterns. 





# Acknoledgement

-   Take-home Exercise 3 by NOR AISYAH BINTE AJIT.
-   In-class Exercise 9 by Prof Kam \[https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex09/in-class_ex09_gwml\]




