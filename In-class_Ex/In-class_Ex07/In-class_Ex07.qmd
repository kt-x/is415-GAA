---
title: "In-class_Ex07"
date: "20 Feb 2023"
date-modified: "`r Sys.Date()`"
number-sections: true
format: html
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
---
# Overview

## Prep packages
```{r}
pacman::p_load(sf, tmap, sfdep, tidyverse)
```

## geospatial 
```{r}
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

## aspatial
```{r}
hunan2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

## left-join
```{r}
# ensure there are same number of observations
hunan_GDPPC <- left_join(hunan,hunan2012) %>%
  select(1:4, 7, 15)
```
# Global measures of spatial association

## Deriving contiguity weights: queen's method
```{r}
wm_q <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_weights(nb, 
                         style = "W"), 
         .before = 1)
```


## Compute Global Moran I
```{r}
muranI <- global_moran(wm_q$GDPPC, 
                       wm_q$nb, 
                       wm_q$wt)
```

## perform moran I
```{r}
global_moran_test(wm_q$GDPPC, 
                  wm_q$nb, 
                  wm_q$wt)
```
CONCLUSION
reject the null hypothesis, the observations have spatial dependence
- if alternative hypothesis is lesser, meaning the moran I stats is negative (the 0.300749970 is positive thats why its greater)


## permutation
to set the value in place cause permutation will keep changing
```{r}
set.seed(1234)
```


```{r}
global_moran_perm(wm_q$GDPPC, 
                  wm_q$nb, 
                  wm_q$wt, 
                  nsim=99)
```

- increase nsim for smaller no. of observations


## LISA
```{r}
lisa <- wm_q |>
  mutate(local_moran = local_moran(
    GDPPC, nb, wt, nsim = 99),
    .before = 1) |>
  unnest(local_moran)
lisa
```

- use mean for take hom ex2
- use p_ii_sim as the use


```{r}
tmap_mode("plot")
tm_shape(lisa) + 
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) + 
  tm_view(set.zoom.limits = c(6, 8))
```


```{r}
tmap_mode("plot")
tm_shape(lisa) + 
  tm_fill("p_ii_sim") + 
  tm_borders(alpha = 0.5)
```


```{r}
lisa_sig <- lisa |> filter(p_ii < 0.05)

tmap_mode("plot")
tm_shape(lisa) + 
  tm_polygons() + 
  tm_borders(alpha = 0.5) + 
tm_shape(lisa_sig) + 
  tm_fill("mean") + 
  tm_borders(alpha = 0.4)
```

- no need lisa for take home ex 2
- use G stats instead of local (?)

- the above plot should have another category for Non-signifcant
(ref to the chap 9/10 somewhere)



## Computing local muran I
```{r}
HCSA <- wm_q |>
  mutate(local_Gi = local_gstar_perm(
    GDPPC, nb, wt, nsim=99
  ), .before = 1) |>
  unnest(local_Gi)

HCSA
```

## visualising Gi
```{r}
tmap_mode("plot")
tm_shape(HCSA) + 
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.5) + 
  tm_view(set.zoom.limits = c(6, 8))
```

## visualising p value of HCSA 

```{r}
tmap_mode("plot")
tm_shape(HCSA) + 
  tm_fill("p_sim") + 
  tm_borders(alpha = 0.5)
```

- 




# hot spot and cold spot

```{r}
#pacman::p_load(sf, tmap, sfdep, tidyverse, plotly)
```


## create a time series cube
```{r}
# GDPPC_st <- space(GDPPC, hunan, 
#                   .loc_col = "County", 
#                   .time_col = "Year")
```


```{r}
# GDPPC_NB <- GDPPC_st |>
#   activate("geometry") |>
#   mutate(
#     nb = include_self(st_contiguity(geometry)), 
#     wt = st_weights(nb)
#   )|>
#   set_nbs("nb") |>
#   set_wts("wt")
```




